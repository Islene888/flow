import urllib.parse
from sqlalchemy import create_engine, text
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
from sqlalchemy.exc import SQLAlchemyError

# âœ… è§£å†³ Matplotlib ä¸­æ–‡ä¹±ç 
matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

# å¯¹å¯†ç è¿›è¡Œ URL ç¼–ç 
password = urllib.parse.quote_plus("flowgpt@2024.com")

# æ„é€ æ•°æ®åº“è¿æ¥ URL
DATABASE_URL = f"mysql+pymysql://bigdata:{password}@18.188.196.105:9030/flow_test"

# åˆ›å»ºæ•°æ®åº“è¿æ¥
engine = create_engine(DATABASE_URL)

# SQL æŸ¥è¯¢ï¼ˆåˆ›å»ºæ–°è¡¨ - ç”¨æˆ·ç•™å­˜ç‡ç»“æœè¡¨ï¼‰
create_table_query = """
CREATE TABLE IF NOT EXISTS tbl_user_engagement_filtered2 (
    dt DATE,
    variation VARCHAR(255),
    users INT,
    d1 INT,
    d2 INT,
    d3 INT,
    d4 INT,
    d5 INT,
    d6 INT,
    d7 INT,
    d8 INT,
    d9 INT,
    d10 INT,
    d11 INT,
    d12 INT,
    d13 INT,
    d14 INT,
    d15 INT,
    d16 INT
);

CREATE TABLE IF NOT EXISTS tbl_new_retention_results2 (
    dt DATE,
    day INT,
    variation VARCHAR(255),
    users INT,
    retained INT,
    retention_rate DOUBLE,
    ci_lower DOUBLE,
    ci_upper DOUBLE,
    control_rate DOUBLE,
    exp_rate DOUBLE,
    uplift DOUBLE,  -- æ·»åŠ  uplift åˆ—
    uplift_ci_lower DOUBLE,  -- æ·»åŠ  uplift_ci_lower åˆ—
    uplift_ci_upper DOUBLE,  -- æ·»åŠ  uplift_ci_upper åˆ—
    z_score DOUBLE,  -- æ·»åŠ  z_score åˆ—
    p_value DOUBLE,  -- æ·»åŠ  p_value åˆ—
    retention_rate_baseline DOUBLE
);
"""

# æ‰§è¡ŒæŸ¥è¯¢å¹¶åˆ›å»ºè¡¨
with engine.connect() as conn:
    conn.execute(text(create_table_query))

# æ‰§è¡Œæ’å…¥æŸ¥è¯¢
insert_query = """
INSERT INTO tbl_user_engagement_filtered2 (dt, variation, users, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15, d16)
SELECT 
    u.first_visit_date AS dt, 
    e.variation, 
    COUNT(DISTINCT u.user_id) AS users,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 1 DAY) THEN a.user_id END) AS d1,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 2 DAY) THEN a.user_id END) AS d2,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 3 DAY) THEN a.user_id END) AS d3,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 4 DAY) THEN a.user_id END) AS d4,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 5 DAY) THEN a.user_id END) AS d5,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 6 DAY) THEN a.user_id END) AS d6,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 7 DAY) THEN a.user_id END) AS d7,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 8 DAY) THEN a.user_id END) AS d8,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 9 DAY) THEN a.user_id END) AS d9,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 10 DAY) THEN a.user_id END) AS d10,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 11 DAY) THEN a.user_id END) AS d11,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 12 DAY) THEN a.user_id END) AS d12,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 13 DAY) THEN a.user_id END) AS d13,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 14 DAY) THEN a.user_id END) AS d14,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 15 DAY) THEN a.user_id END) AS d15,
    COUNT(DISTINCT CASE WHEN a.active_date = DATE_ADD(u.first_visit_date, INTERVAL 16 DAY) THEN a.user_id END) AS d16
FROM
    (SELECT
        user_id,
        DATE(first_visit_date) AS first_visit_date
    FROM
        flow_wide_info.tbl_wide_user_first_visit_app_info
    WHERE
        first_visit_date BETWEEN '2024-12-17' AND '2025-01-02') u
LEFT JOIN
    (SELECT
        u.user_id,
        u.first_visit_date,
        DATE(FROM_UNIXTIME(a.ingest_timestamp / 1000, '%Y-%m-%d')) AS active_date
    FROM
        flow_wide_info.tbl_wide_user_first_visit_app_info u
    JOIN
        flow_wide_info.tbl_wide_backend_detail_hi a ON u.user_id = a.user_id
    WHERE
        a.event_name = 'Chat_LLM'
        AND a.device_type = 'MOBILE'
        AND DATE(FROM_UNIXTIME(a.ingest_timestamp / 1000, '%Y-%m-%d')) BETWEEN u.first_visit_date AND '2025-01-02') a
ON u.user_id = a.user_id
LEFT JOIN
    (SELECT
        user_id,
        CAST(variation_id AS CHAR) AS variation
    FROM
        flow_wide_info.tbl_wide_experiment_assignment_hi
    WHERE
        experiment_id = 'mobile-non-claude-11'
        AND timestamp_assigned BETWEEN '2024-12-17 12:00:00' AND '2025-01-02 12:00:00') e
ON u.user_id = e.user_id
GROUP BY
    u.first_visit_date, e.variation
ORDER BY 
    u.first_visit_date;
"""

# æ‰§è¡ŒæŸ¥è¯¢å¹¶æ’å…¥æ•°æ®
with engine.connect() as conn:
    conn.execute(text(insert_query))

print("SQL query executed successfully.")

# è¿è¡Œ SQL æŸ¥è¯¢ï¼Œè·å–æ•°æ®
query = """
SELECT * FROM tbl_user_engagement_filtered2
ORDER BY dt ASC, CAST(variation AS UNSIGNED) ASC;
"""
df = pd.read_sql(query, engine)

# ç¡®ä¿ `variation` æ˜¯å­—ç¬¦ä¸²ç±»å‹
df["variation"] = df["variation"].astype(str)

# è®¡ç®—æ¯å¤©çš„ç•™å­˜ç‡
days = ["d1", "d2", "d3", "d4", "d5", "d6", "d7", "d8", "d9", "d10", "d11", "d12", "d13", "d14", "d15"]

# æ›¿æ¢ "d1", "d2", ..., "d15" ä¸ºå¯¹åº”çš„é˜¿æ‹‰ä¼¯æ•°å­— 1, 2, ..., 15
day_map = {f"d{i}": str(i) for i in range(1, 16)}

results = []

for _, row in df.iterrows():
    dt = row["dt"]
    variation = row["variation"]
    users = row["users"]  # è¯¥å®éªŒç»„çš„æ–°å¢ç”¨æˆ·æ•°

    if users > 0:
        for day in days:
            # æ›¿æ¢ 'd1', 'd2', ..., 'd15' ä¸ºæ•°å­—
            day_num = day_map[day]
            retained = row[day]
            retention_rate = retained / users if users > 0 else 0
            se = np.sqrt((retention_rate * (1 - retention_rate)) / users) if users > 0 else 0  # æ ‡å‡†è¯¯

            ci_lower = retention_rate - 1.96 * se
            ci_upper = retention_rate + 1.96 * se

            results.append({
                "dt": dt,
                "day": day_num,  # ä½¿ç”¨æ•°å­—
                "variation": variation,
                "users": users,
                "retained": retained,
                "retention_rate": round(retention_rate, 4),
                "ci_lower": round(ci_lower, 4),
                "ci_upper": round(ci_upper, 4)
            })

# è½¬æ¢ä¸º DataFrame
result_df = pd.DataFrame(results)

# è®¡ç®—å®éªŒç»„ vs å¯¹ç…§ç»„çš„å¢é•¿ç‡åŠç½®ä¿¡åŒºé—´
control_df = result_df[result_df["variation"] == "0"]
experiment_df = result_df[result_df["variation"] != "0"]

comparison_results = []
for day in range(1, 16):  # ä½¿ç”¨æ•°å­— 1 åˆ° 15
    for dt in result_df["dt"].unique():
        control_row = control_df[(control_df["day"] == str(day)) & (control_df["dt"] == dt)]
        if control_row.empty:
            continue

        control_rate = control_row["retention_rate"].values[0]
        control_se = np.sqrt((control_rate * (1 - control_rate)) / control_row["users"].values[0]) if \
        control_row["users"].values[0] > 0 else 0

        for variation in experiment_df["variation"].unique():
            exp_row = experiment_df[
                (experiment_df["day"] == str(day)) & (experiment_df["variation"] == variation) & (experiment_df["dt"] == dt)]
            if exp_row.empty:
                continue

            exp_rate = exp_row["retention_rate"].values[0]
            exp_se = np.sqrt((exp_rate * (1 - exp_rate)) / exp_row["users"].values[0]) if exp_row["users"].values[
                                                                                              0] > 0 else 0

            if control_rate == 0 or exp_rate == 0:
                uplift, uplift_se, z_score, p_value, uplift_lower, uplift_upper = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan
            else:
                uplift = (exp_rate - control_rate) / control_rate
                uplift_se = np.sqrt((control_se / control_rate) ** 2 + (exp_se / exp_rate) ** 2) if control_rate > 0 and exp_rate > 0 else 0

                uplift_lower = uplift - 1.96 * uplift_se
                uplift_upper = uplift + 1.96 * uplift_se

                z_score = (exp_rate - control_rate) / np.sqrt(control_se ** 2 + exp_se ** 2) if (control_se ** 2 + exp_se ** 2) > 0 else np.nan
                p_value = 2 * (1 - stats.norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan

            comparison_results.append({
                "dt": dt,
                "day": str(day),  # ä½¿ç”¨æ•°å­—
                "variation": variation,
                "control_rate": round(control_rate, 4),
                "exp_rate": round(exp_rate, 4),
                "uplift": round(uplift, 4) if not np.isnan(uplift) else np.nan,
                "uplift_ci_lower": round(uplift_lower, 4) if not np.isnan(uplift_lower) else np.nan,
                "uplift_ci_upper": round(uplift_upper, 4) if not np.isnan(uplift_upper) else np.nan,
                "z_score": round(z_score, 4) if not np.isnan(z_score) else np.nan,
                "p_value": round(p_value, 4) if not np.isnan(p_value) else np.nan
            })

# è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜
comparison_df = pd.DataFrame(comparison_results)

# åˆå¹¶ä¸¤ä¸ª DataFrameï¼Œæ·»åŠ æ¯”è¾ƒç»“æœåˆ°åŸç»“æœä¸­
final_df = pd.merge(result_df, comparison_df, on=["dt", "day", "variation"], how="left")

# åˆ›å»ºä¸€ä¸ªæ–°çš„ DataFrameï¼Œä»…åŒ…å« variation ä¸º "0" çš„è¡Œï¼ˆå³å¯¹ç…§ç»„ï¼‰
control_df = final_df[final_df["variation"] == "0"]

# åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ— 'baseline_retention_rate'ï¼Œå°†æ¯ä¸ªæ—¥æœŸå’Œå¤©æ•°ï¼ˆdayï¼‰å¯¹åº”çš„å¯¹ç…§ç»„ç•™å­˜ç‡æ·»åŠ åˆ°æ‰€æœ‰å˜ä½“ä¸­
baseline_retention_rate = control_df[["dt", "day", "retention_rate"]].drop_duplicates()

# åˆå¹¶ baseline_retention_rate åˆ°åŸå§‹ç»“æœä¸­ï¼ŒæŒ‰ç…§ 'dt' å’Œ 'day' è¿›è¡Œåˆå¹¶
final_df = pd.merge(final_df, baseline_retention_rate, on=["dt", "day"], how="left", suffixes=("", "_baseline"))

# å¤„ç† 'N/A' å’Œç±»å‹è½¬æ¢
final_df = final_df.replace("N/A", np.nan)

# è½¬æ¢ 'N/A' ä¸º Noneï¼Œå¹¶è¿›è¡Œç±»å‹è½¬æ¢
final_df = final_df.where(pd.notnull(final_df), None)

# å¤„ç†æ•°æ®ç±»å‹
numeric_cols = ["uplift", "uplift_ci_lower", "uplift_ci_upper", "p_value"]
for col in numeric_cols:
    final_df[col] = pd.to_numeric(final_df[col], errors='coerce')

# ä¿å­˜æœ€ç»ˆç»“æœåˆ° CSVï¼ˆå¯é€‰ï¼‰
final_df.to_csv("final_retention_results2.csv", index=False)
print("âœ… A/B æµ‹è¯•æœ€ç»ˆç»“æœå·²ä¿å­˜ï¼")

# å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
plt.figure(figsize=(12, 6))
sns.lineplot(data=final_df, x="day", y="retention_rate", hue="variation", marker="o")
plt.axhline(control_df["retention_rate"].mean(), linestyle="--", color="black", linewidth=2, label="Control Baseline")
plt.title("å®éªŒç»„ vs å¯¹ç…§ç»„çš„ç•™å­˜ç‡")
plt.xlabel("å¤©æ•°")
plt.ylabel("ç•™å­˜ç‡")
plt.legend()
plt.show()

# å°† final_df æ’å…¥åˆ°æ•°æ®åº“ä¸­çš„ tbl_new_retention_results (ä½¿ç”¨ append æ¨¡å¼)
try:
    final_df.to_sql(
        name='tbl_new_retention_results2',
        con=engine,
        if_exists='append',
        index=False,
        method='multi',  # æ‰¹é‡æ’å…¥æå‡æ€§èƒ½
        chunksize=500  # æ ¹æ®éœ€è¦è°ƒæ•´æ‰¹æ¬¡å¤§å°
    )
    print("âœ… æ•°æ®å·²æˆåŠŸæ’å…¥åˆ°æ•°æ®åº“ï¼")
except SQLAlchemyError as e:
    print(f"ğŸš¨ æ•°æ®åº“æ’å…¥å¤±è´¥: {e}")
