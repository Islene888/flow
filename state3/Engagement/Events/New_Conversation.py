import urllib.parse
import pandas as pd
from sqlalchemy import create_engine, text
import warnings
from datetime import datetime, timedelta
import logging

from state2.growthbook_fetcher.experiment_tag_all_parameters import get_experiment_details_by_tag

warnings.filterwarnings("ignore", category=FutureWarning)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


# ============= æ•°æ®åº“è¿æ¥ =============
def get_db_connection():
    password = urllib.parse.quote_plus("flowgpt@2024.com")
    DATABASE_URL = f"mysql+pymysql://bigdata:{password}@3.135.224.186:9030/flow_ab_test?charset=utf8mb4"
    engine = create_engine(DATABASE_URL, pool_recycle=3600)
    logging.info("âœ… æ•°æ®åº“è¿æ¥å·²å»ºç«‹ã€‚")
    return engine


# ============= æŒ‰å¤© & åˆ†ç‰‡æ’å…¥æ•°æ® =============
def insert_new_conversation_data(tag):
    logging.info(f"ğŸš€ å¼€å§‹è·å–å®éªŒæ•°æ®ï¼Œæ ‡ç­¾ï¼š{tag}")
    experiment_data = get_experiment_details_by_tag(tag)
    if not experiment_data:
        logging.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ ‡ç­¾ '{tag}' çš„å®éªŒæ•°æ®ï¼")
        return None

    experiment_name = experiment_data['experiment_name']
    start_time = experiment_data['phase_start_time']
    end_time = experiment_data['phase_end_time']

    logging.info(f"ğŸ“ å®éªŒåç§°ï¼š{experiment_name}ï¼Œå®éªŒæ—¶é—´ï¼š{start_time} è‡³ {end_time}")

    engine = get_db_connection()
    table_name = f"tbl_report_new_conversation_{tag}"

    create_table_query = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        variation VARCHAR(255),
        total_new_conversation INT,
        unique_new_conversation_users INT,
        new_conversation_ratio DOUBLE,
        experiment_name VARCHAR(255)
    );
    """
    truncate_query = f"TRUNCATE TABLE {table_name};"

    with engine.connect() as conn:
        conn.execute(text("SET query_timeout = 30000;"))
        conn.execute(text(create_table_query))
        conn.execute(text(truncate_query))
        logging.info(f"âœ… ç›®æ ‡è¡¨ {table_name} å·²åˆ›å»ºï¼Œå¹¶æ¸…ç©ºæ•°æ®ã€‚")

        current_date = start_time
        while current_date <= end_time:
            date_str = current_date.strftime('%Y-%m-%d')
            logging.info(f"ğŸ“… å¤„ç†æ—¥æœŸï¼š{date_str}")

            for batch_index in range(10):  # 10 åˆ†ç‰‡
                batch_insert_query = text(f"""
                INSERT INTO {table_name} (variation, total_new_conversation, unique_new_conversation_users, new_conversation_ratio, experiment_name)
                SELECT 
                    a.variation_id AS variation,
                    COUNT(DISTINCT c.conversation_id) AS total_new_conversation,
                    COUNT(DISTINCT c.user_id) AS unique_new_conversation_users,
                    CASE 
                        WHEN COUNT(DISTINCT c.user_id) = 0 THEN 0 
                        ELSE ROUND(COUNT(DISTINCT c.conversation_id) / COUNT(DISTINCT c.user_id), 4) 
                    END AS new_conversation_ratio,
                    :experiment_name AS experiment_name
                FROM flow_event_info.tbl_app_event_chat_send c
                JOIN flow_wide_info.tbl_wide_experiment_assignment_hi a
                    ON c.user_id = a.user_id
                WHERE a.experiment_id = :experiment_name
                  AND c.ingest_timestamp >= :start_time
                  AND c.ingest_timestamp < :end_time
                  AND c.conversation_length = 1
                  AND MOD(crc32(c.user_id), 10) = :batch_index
                GROUP BY a.variation_id;
                """)

                conn.execute(batch_insert_query, {
                    "experiment_name": experiment_name,
                    "start_time": f"{date_str} 00:00:00",
                    "end_time": f"{date_str} 23:59:59",
                    "batch_index": batch_index
                })
                logging.info(f"âœ… æ—¥æœŸ {date_str}ï¼Œæ‰¹æ¬¡ {batch_index}/10 æ’å…¥å®Œæˆã€‚")

            current_date += timedelta(days=1)

    logging.info(f"âœ… æ‰€æœ‰æ•°æ®æ’å…¥å®Œæˆï¼Œç›®æ ‡è¡¨ï¼š{table_name}")
    return table_name


# ============= è®¡ç®—æ±‡æ€»å¹¶è¦†ç›–åŸè¡¨ =============
def overwrite_new_conversation_table_with_summary(tag):
    logging.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆæ±‡æ€»æ•°æ®ï¼Œå¹¶è¦†ç›–åˆ°åŸè¡¨ï¼Œæ ‡ç­¾ï¼š{tag}")

    table_name = f"tbl_report_new_conversation_{tag}"
    engine = get_db_connection()

    summary_query = text(f"""
    SELECT 
        variation,
        SUM(total_new_conversation) AS total_new_conversation,
        SUM(unique_new_conversation_users) AS unique_new_conversation_users,
        CASE 
            WHEN SUM(unique_new_conversation_users) = 0 THEN 0
            ELSE ROUND(SUM(total_new_conversation) / SUM(unique_new_conversation_users), 4)
        END AS new_conversation_ratio,
        MAX(experiment_name) AS experiment_name
    FROM {table_name}
    GROUP BY variation;
    """)

    summary_df = pd.read_sql(summary_query, engine)
    summary_df.to_sql(table_name, engine, if_exists="replace", index=False)

    logging.info(f"âœ… æ±‡æ€»æ•°æ®å·²è¦†ç›–è¡¨ï¼š{table_name}")

if __name__ == "__main__":
    main("backend")